{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate quantization on a simple model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/anaconda3/lib/python311.zip', '/opt/anaconda3/lib/python3.11', '/opt/anaconda3/lib/python3.11/lib-dynload', '', '/Users/deyucao/Library/Caches/pypoetry/virtualenvs/quantization-ubRKDRCl-py3.11/lib/python3.11/site-packages', '/var/folders/rx/ym13fcm14d568j0djggdjsrh0000gn/T/tmptwlyia6c', '/Users/deyucao/Quantization', '/Users/deyucao/Quantization', '/Users/deyucao/Quantization']\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Add the parent directory to the path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "print(sys.path)\n",
    "\n",
    "from core.quantization import quantize_model\n",
    "from core.models.easy_quant import EasyQuantConfig\n",
    "from core.models.squeeze_llm import SqueezeQuantConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a simple model with only FC layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To keep the quantization process simple, I only used linear layers, which is sufficient for a simple task like MNIST classification.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.5171\n",
      "Epoch [2/5], Loss: 0.3792\n",
      "Epoch [3/5], Loss: 0.3437\n",
      "Epoch [4/5], Loss: 0.3174\n",
      "Epoch [5/5], Loss: 0.3015\n",
      "Test Accuracy: 86.73%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "cal_dataset_size = 1000\n",
    "\n",
    "# Dataset preprocessing\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# Loading the MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, transform=transform, download=True\n",
    ")\n",
    "test_and_cal_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Split the test dataset into test and calibration datasets\n",
    "test_dataset, cal_dataset = random_split(\n",
    "    test_and_cal_dataset, [10000 - cal_dataset_size, cal_dataset_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "cal_loader = DataLoader(dataset=cal_dataset, batch_size=cal_dataset_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Convert 28x28 image to 1D\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleLinearModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Model testing\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLinearModel(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and store the gradients of the loss for the calibration dataset\n",
    "model.eval()\n",
    "\n",
    "# Get the gradients of the loss w.r.t. the weights\n",
    "for images, labels in cal_loader:\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1, Module: Linear(in_features=784, out_features=128, bias=True)\n",
      "Layer: fc2, Module: Linear(in_features=128, out_features=64, bias=True)\n",
      "Layer: fc3, Module: Linear(in_features=64, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_children():\n",
    "    print(f\"Layer: {name}, Module: {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Model ready for use from epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rx/ym13fcm14d568j0djggdjsrh0000gn/T/ipykernel_840/2738249724.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    print(f\"Checkpoint loaded. Model ready for use from epoch {start_epoch}.\")\n",
    "else:\n",
    "    print(\"No checkpoint found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_and_predict(model: nn.Module, image: torch.Tensor) -> tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Display an input image and predict the digit using a given model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model for digit classification.\n",
    "        image (torch.Tensor): The input image tensor of shape (1, 28, 28).\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, float]: Predicted digit and the confidence (probability) of the prediction.\n",
    "    \"\"\"\n",
    "    # Display the input image\n",
    "    plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Preprocess the image (add batch dimension and flatten)\n",
    "    image = image.view(-1, 28 * 28)  # Shape (1, 784)\n",
    "\n",
    "    # Set model to evaluation mode and make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_label].item()\n",
    "\n",
    "    print(f\"Predicted Label: {predicted_label}, Confidence: {confidence:.4f}\")\n",
    "    return predicted_label, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on a test dataset and return the accuracy.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model for digit classification.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 2, Confidence: 0.9462\n",
      "True Label: 2\n"
     ]
    }
   ],
   "source": [
    "# Load a sample image from the test set\n",
    "image, label = test_dataset[0]\n",
    "\n",
    "# Display the image and make a prediction\n",
    "predicted_label, confidence = display_and_predict(model, image)\n",
    "print(f\"True Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model with naive method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, all the quantization methods use 3-bit representation unless otherwise noted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model_naive(model: nn.Module, num_bits: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Naively quantize a model to the specified number of bits.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to quantize.\n",
    "        num_bits (int): The number of bits to use for quantization.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The quantized model.\n",
    "    \"\"\"\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    scale = 2 ** (num_bits - 1)\n",
    "\n",
    "    for (name, module), (name_q, module_q) in zip(\n",
    "        model.named_modules(), quantized_model.named_modules()\n",
    "    ):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            min_val, max_val = module.weight.data.min(), module.weight.data.max()\n",
    "            print(f\"Layer: {name}, Min: {min_val}, Max: {max_val}\")\n",
    "            print(f\"Quantization range is: {(max_val - min_val) / scale}\")\n",
    "            module_q.weight.data = (\n",
    "                torch.round(module.weight.data / ((max_val - min_val) / scale))\n",
    "                * ((max_val - min_val) / scale)\n",
    "                + min_val\n",
    "            )\n",
    "\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1, Min: -0.4861774742603302, Max: 0.5386022925376892\n",
      "Quantization range is: 0.25619494915008545\n",
      "Layer: fc2, Min: -0.40028369426727295, Max: 0.34358125925064087\n",
      "Quantization range is: 0.18596623837947845\n",
      "Layer: fc3, Min: -0.5346959233283997, Max: 0.35027244687080383\n",
      "Quantization range is: 0.22124210000038147\n"
     ]
    }
   ],
   "source": [
    "quantized_model_naive = quantize_model_naive(model, num_bits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1\n",
      "Original Weights: tensor([[ 4.4953e-02,  1.4459e-02,  1.3202e-02,  7.3505e-02,  6.6165e-02,\n",
      "          7.0585e-02,  3.3076e-02,  5.9530e-02, -1.0409e-04,  1.9414e-02,\n",
      "          8.9731e-03,  2.3098e-02,  1.1156e-02, -5.7074e-02,  1.4315e-03,\n",
      "         -3.6295e-02, -9.3052e-03,  5.4284e-02,  9.8698e-02,  5.4676e-02,\n",
      "          4.4590e-02,  9.0425e-02,  6.7728e-02,  3.8124e-02,  2.0636e-02,\n",
      "          4.7400e-02,  4.0564e-02,  6.7536e-02,  6.5369e-02,  3.0069e-02,\n",
      "          2.2998e-02,  9.0965e-03,  2.3003e-03,  4.5294e-02,  1.2688e-02,\n",
      "          7.3098e-02,  3.2446e-02,  1.7839e-02,  4.6168e-03, -4.7986e-02,\n",
      "         -7.3794e-02, -1.0078e-01, -4.3962e-02, -7.9742e-02, -4.1934e-02,\n",
      "         -1.1022e-03,  6.4290e-02,  6.6511e-02,  9.9387e-02,  8.3007e-02,\n",
      "          1.8057e-02,  5.6577e-02,  4.9880e-02,  1.7001e-02,  4.9559e-02,\n",
      "          5.1400e-02,  7.2897e-02,  5.1298e-02,  2.1745e-02,  5.2792e-02,\n",
      "          5.4903e-02,  2.7506e-02,  3.9348e-02,  3.7462e-02,  2.1254e-02,\n",
      "         -4.9688e-02, -6.2048e-02, -4.9616e-03, -2.5470e-02, -8.6561e-02,\n",
      "         -3.7331e-02, -5.4948e-03, -2.7692e-02, -1.2007e-02, -1.0386e-02,\n",
      "         -1.6743e-02,  2.5843e-02,  4.0244e-02,  4.2486e-02,  3.8474e-02,\n",
      "          7.1004e-02,  6.0323e-02,  5.0512e-02,  1.4613e-02,  3.1428e-02,\n",
      "          3.3452e-02,  2.8592e-02,  6.1879e-02,  2.1312e-02,  2.8313e-02,\n",
      "          3.2250e-02, -3.0846e-02,  4.8456e-03, -1.8242e-02, -4.2251e-02,\n",
      "         -4.2474e-02, -4.3256e-02, -3.5547e-02, -5.2500e-02, -4.3682e-02,\n",
      "         -3.4941e-03,  1.7051e-02, -5.6905e-03, -6.1958e-03, -1.3064e-02,\n",
      "          5.3290e-04,  3.8930e-02,  1.8239e-02,  5.1442e-02,  4.5399e-02,\n",
      "          8.3525e-02,  7.5142e-02,  5.8318e-03,  6.3806e-02,  5.6993e-02,\n",
      "         -3.1897e-03,  6.3922e-03, -3.0095e-02,  9.4412e-03,  4.7030e-03,\n",
      "         -4.5177e-02, -9.0031e-03, -8.1593e-02, -7.2437e-02, -5.6540e-02,\n",
      "         -5.8510e-02, -8.8545e-02, -4.2479e-02, -2.5882e-03, -3.4434e-03,\n",
      "          6.8049e-02, -8.7448e-03,  4.1499e-02,  3.5858e-02,  5.4234e-02,\n",
      "          5.9226e-02,  4.0896e-02,  2.9333e-02,  4.1634e-02,  6.0256e-02,\n",
      "          2.3671e-02,  4.8890e-02,  4.6633e-02,  3.7196e-02, -1.3444e-02,\n",
      "         -2.9500e-02, -4.0967e-03, -5.6777e-02, -6.1942e-02, -6.2694e-02,\n",
      "         -5.4224e-02, -8.1323e-02, -3.8445e-02, -7.3685e-02, -5.2594e-02,\n",
      "          1.9883e-02,  5.1051e-02,  4.8926e-02,  1.1513e-01,  6.6202e-02,\n",
      "          1.8518e-02,  4.9186e-03,  2.9081e-02,  1.1724e-02,  2.2897e-02,\n",
      "          1.0059e-02,  2.8025e-02,  3.4675e-02,  4.5707e-02,  8.8424e-03,\n",
      "         -1.8273e-02,  1.6756e-02, -5.4643e-02, -3.1417e-03, -7.0060e-02,\n",
      "         -2.4534e-02, -4.5566e-02, -5.6890e-02, -1.0499e-01, -6.7252e-02,\n",
      "         -3.0984e-02, -1.2733e-02, -2.9179e-02,  2.0385e-02,  1.0462e-02,\n",
      "          3.1438e-02,  1.0620e-01,  4.1025e-02,  2.0246e-02, -5.8698e-02,\n",
      "         -1.1592e-02, -2.9939e-02, -3.6236e-02,  1.0937e-03,  4.7256e-02,\n",
      "          4.3358e-02,  3.0788e-02, -1.1184e-02, -9.0102e-03,  1.0226e-02,\n",
      "         -1.6386e-02, -3.0696e-02, -8.1845e-02, -7.3458e-02, -6.1368e-02,\n",
      "         -1.0282e-01, -1.4969e-01, -1.3854e-01, -9.1920e-02, -1.1604e-01,\n",
      "         -8.3234e-02, -1.6629e-02,  1.7343e-02,  2.9827e-02,  7.6503e-02,\n",
      "          1.2113e-01,  1.3821e-02, -6.9123e-02, -7.1219e-02, -6.1875e-03,\n",
      "         -5.2199e-02, -3.2387e-02, -3.5634e-02,  1.1922e-02,  5.5929e-02,\n",
      "         -3.9530e-02, -1.8369e-03, -6.2060e-02, -2.3905e-02, -7.0768e-02,\n",
      "         -1.0758e-01, -9.0782e-02, -9.7095e-02, -1.3152e-01, -1.3203e-01,\n",
      "         -1.2832e-01, -1.9413e-01, -2.1752e-01, -1.5740e-01, -4.6008e-02,\n",
      "          6.8846e-02,  1.0166e-01,  6.3478e-02,  5.5396e-03, -4.1983e-02,\n",
      "         -1.1705e-01, -3.3160e-02, -7.3755e-02,  1.9208e-02, -2.3004e-02,\n",
      "         -2.6496e-02,  3.8552e-02,  2.1864e-02, -5.0932e-02, -6.3900e-02,\n",
      "         -4.7014e-02, -5.1120e-02, -8.8253e-02, -7.5612e-02, -8.6519e-02,\n",
      "         -1.3829e-01, -1.9318e-01, -2.0434e-01, -2.3572e-01, -2.0565e-01,\n",
      "         -1.7897e-01, -7.4103e-02, -2.7812e-02,  1.5913e-01,  2.1795e-01,\n",
      "          1.2189e-01, -4.6797e-02, -1.3842e-01, -1.0656e-01, -1.2407e-01,\n",
      "          9.8189e-03,  2.3099e-02,  5.5080e-02,  4.6023e-03, -2.5231e-02,\n",
      "          2.7087e-02, -4.7202e-02, -1.0687e-01, -1.0546e-01, -8.3800e-02,\n",
      "         -1.0729e-01, -1.5506e-01, -1.4229e-01, -1.9565e-01, -2.0298e-01,\n",
      "         -2.5315e-01, -2.1377e-01, -1.5859e-01, -1.1930e-01,  6.4688e-02,\n",
      "          8.6884e-02,  2.4470e-02,  1.2444e-01,  9.5794e-02,  3.9585e-02,\n",
      "         -2.6949e-02, -5.2334e-02, -4.5871e-03,  1.9305e-02,  2.7322e-02,\n",
      "          1.7112e-01,  7.0125e-02, -6.0414e-02, -1.4591e-02, -6.5620e-02,\n",
      "         -8.0851e-02, -1.0933e-01, -1.2430e-01, -1.3723e-01, -1.8613e-01,\n",
      "         -2.0035e-01, -1.4188e-01, -1.7117e-01, -6.3009e-02,  1.9332e-02,\n",
      "         -6.8798e-02, -4.6863e-02,  6.0671e-03,  9.5041e-03,  4.0794e-02,\n",
      "          2.8286e-02,  9.0364e-02,  5.8955e-02,  6.3850e-02,  2.4037e-03,\n",
      "          3.5280e-02, -2.9994e-02,  4.5516e-02,  1.7692e-01,  1.2325e-01,\n",
      "         -3.9406e-02, -1.4758e-03, -8.7952e-02, -7.5124e-02, -7.4009e-02,\n",
      "         -8.8671e-02, -1.2919e-01, -1.4685e-01, -1.6221e-01, -9.0510e-02,\n",
      "         -5.5712e-02, -4.2681e-02, -1.2957e-01, -1.2130e-01, -6.1533e-02,\n",
      "          1.9375e-02, -5.4988e-03, -5.3401e-02,  2.9956e-02,  4.6585e-02,\n",
      "          8.5118e-02,  5.8434e-02, -3.2559e-02,  7.8184e-02,  5.6411e-03,\n",
      "          3.9015e-03,  7.8645e-02,  2.1338e-01, -7.3955e-03, -7.2313e-02,\n",
      "         -1.1775e-01, -3.3968e-02,  6.7760e-03, -6.8368e-02, -1.9274e-01,\n",
      "         -1.6186e-01,  1.7150e-02, -5.0710e-02, -5.1742e-02, -1.2751e-02,\n",
      "          1.4917e-02, -1.0203e-01, -7.1985e-02, -8.0622e-02,  7.6139e-03,\n",
      "         -5.8230e-02, -9.6227e-02,  3.4811e-02,  8.8581e-02,  1.7809e-02,\n",
      "          5.8893e-02,  4.6704e-03,  8.3398e-02,  5.6938e-02,  7.1535e-02,\n",
      "          2.0683e-01,  3.3385e-02, -5.2863e-02, -1.9614e-02,  9.4199e-02,\n",
      "          2.3507e-02,  4.8279e-03, -2.2027e-01, -3.8532e-02, -5.9603e-02,\n",
      "         -1.3918e-02, -6.4070e-02, -1.0466e-01, -2.5875e-02, -1.1665e-01,\n",
      "          5.7553e-02, -5.4260e-02, -3.1319e-02,  7.4689e-03, -2.3025e-02,\n",
      "          7.7259e-02,  6.8775e-02, -3.5329e-04,  1.5906e-02, -7.1097e-02,\n",
      "         -3.4880e-02,  6.0928e-02,  1.2399e-01,  1.8412e-01,  9.8868e-03,\n",
      "          5.3438e-02,  1.3422e-01,  1.7665e-01,  1.9455e-01,  1.0889e-01,\n",
      "         -1.3907e-01, -3.2014e-02, -3.5206e-02, -6.4014e-02,  4.3940e-02,\n",
      "          7.7211e-02, -3.2646e-02, -1.3682e-01, -8.1288e-02, -8.8389e-02,\n",
      "         -3.3379e-02, -5.3075e-02,  4.0506e-02, -5.6534e-03, -2.6305e-02,\n",
      "          3.6520e-02,  3.6112e-02,  1.2100e-01,  5.0225e-02,  1.3348e-01,\n",
      "          1.3004e-01,  9.4042e-02,  4.4278e-02,  1.7666e-01,  2.4119e-01,\n",
      "          1.8327e-01,  2.1346e-01,  8.8317e-02, -1.8650e-01, -1.5519e-01,\n",
      "         -1.2656e-01, -6.4254e-02, -2.6598e-02, -3.4463e-02, -6.5917e-02,\n",
      "         -2.9252e-02, -1.3014e-02, -3.1837e-02, -4.4548e-02, -7.1024e-03,\n",
      "          5.0948e-02,  7.6259e-02, -2.0397e-02,  1.6425e-02,  1.0510e-01,\n",
      "         -4.4891e-02,  4.9973e-02,  1.5060e-01,  6.1727e-02,  5.4626e-04,\n",
      "          6.1017e-02,  1.4862e-01,  1.5807e-01,  2.6212e-01,  1.5033e-01,\n",
      "         -4.2261e-02, -1.3364e-01, -1.0378e-01, -6.7033e-02,  6.2724e-02,\n",
      "         -2.8455e-02, -8.6358e-02, -4.8155e-03, -8.6527e-02, -7.2931e-02,\n",
      "         -1.3306e-01, -7.2261e-02, -4.2027e-02,  1.4729e-02, -3.4074e-02,\n",
      "         -1.5047e-02,  6.4730e-02,  4.0182e-02, -6.2975e-02, -5.0930e-02,\n",
      "         -5.9811e-02, -4.2624e-02,  1.5821e-02,  7.3985e-02,  6.4071e-02,\n",
      "          9.8212e-02,  1.9960e-01,  1.1301e-01,  9.1411e-02, -1.1414e-01,\n",
      "         -1.0289e-01, -8.6524e-02,  3.8314e-02,  3.9141e-02, -3.6062e-02,\n",
      "         -9.4197e-02, -1.0039e-01, -8.7818e-02, -4.7154e-02,  4.3437e-02,\n",
      "         -1.6978e-03, -3.3406e-02, -4.2447e-02,  1.4338e-03,  4.8457e-02,\n",
      "          7.9923e-02, -1.9694e-02,  3.1049e-02,  5.6697e-03, -1.9019e-02,\n",
      "          7.3927e-02,  1.2614e-01, -8.8503e-03, -3.4637e-02,  5.4407e-02,\n",
      "          5.1010e-03, -3.9718e-03, -1.4885e-01, -6.9488e-02,  1.4720e-02,\n",
      "          3.0942e-02,  8.1408e-02, -9.4208e-02,  2.9779e-04, -6.8912e-02,\n",
      "          2.0032e-02, -1.1649e-02,  4.2922e-02,  1.9164e-02,  6.2998e-02,\n",
      "          5.6928e-03,  2.7709e-02,  1.1752e-01,  9.2890e-02,  5.0106e-03,\n",
      "         -9.4933e-04,  8.0682e-02, -4.1729e-02,  8.7241e-02,  1.3893e-01,\n",
      "          3.6457e-02,  7.0809e-03,  6.5253e-02,  5.2910e-02, -2.6852e-02,\n",
      "         -8.1051e-02, -1.1608e-01, -1.1664e-01, -6.4242e-02, -7.5727e-02,\n",
      "         -4.0447e-02, -1.0456e-01, -9.6538e-02, -9.3578e-02, -6.0387e-02,\n",
      "          1.9192e-03, -7.4878e-02,  1.0573e-01,  8.1670e-02,  1.2001e-01,\n",
      "          8.3642e-02,  9.7742e-03,  2.9215e-02,  8.4000e-02,  5.0543e-02,\n",
      "         -4.2990e-02,  5.5588e-02,  5.5129e-02,  3.2270e-02, -9.6651e-03,\n",
      "          8.3314e-02,  5.2851e-02, -4.8813e-02, -5.0576e-02, -1.3059e-01,\n",
      "         -9.3429e-02, -1.2164e-01, -4.3751e-02, -1.0037e-01, -1.0674e-01,\n",
      "         -1.5856e-02, -6.1259e-04, -6.1531e-02, -3.0785e-02,  4.2530e-02,\n",
      "          1.0208e-01,  1.0852e-01,  1.3800e-01,  6.0649e-02,  1.2688e-02,\n",
      "         -1.5970e-02,  4.7666e-02,  3.5708e-02,  6.8242e-03,  2.6600e-02,\n",
      "          2.7824e-02,  5.1086e-04,  4.9902e-02,  5.7665e-02,  8.1857e-02,\n",
      "          2.5902e-02, -2.5747e-02, -3.2216e-02, -5.1871e-02, -5.3720e-03,\n",
      "          3.7692e-03,  8.1735e-03, -4.6753e-03, -4.7557e-02, -5.1768e-02,\n",
      "          9.0347e-03,  4.8933e-02,  2.8356e-02,  8.3213e-02,  5.5332e-02,\n",
      "          8.2686e-02,  4.2589e-02,  1.5141e-02, -1.9087e-02,  9.8485e-03,\n",
      "          5.4538e-03, -3.0023e-02,  1.5420e-02,  3.0203e-02,  5.9847e-02,\n",
      "          1.8420e-02,  3.1503e-02,  1.1395e-02, -5.1511e-02, -2.6960e-02,\n",
      "         -9.1816e-02, -7.6200e-02, -6.4515e-02, -4.1778e-02, -6.3194e-02,\n",
      "         -1.7430e-03, -7.0672e-02, -5.6351e-02, -7.6626e-03, -5.4104e-02,\n",
      "          2.3352e-02,  4.2909e-02,  1.5061e-02,  7.5088e-02,  1.6744e-02,\n",
      "          3.5393e-02,  1.0364e-02,  1.0231e-02,  2.9785e-02, -1.4261e-02,\n",
      "          3.8289e-02,  1.5095e-02,  7.0367e-02,  4.5532e-02,  1.2699e-02,\n",
      "          6.0035e-02, -2.3352e-02, -1.7613e-02, -6.8310e-02, -9.4466e-03,\n",
      "         -6.0031e-02, -2.8187e-02, -3.5192e-03, -1.2682e-02, -7.1253e-03,\n",
      "          1.6343e-02, -5.7572e-03, -7.7455e-03,  2.7800e-03, -3.4214e-02,\n",
      "          1.6116e-02,  4.4758e-02,  9.5809e-03,  1.0458e-02,  2.1366e-02,\n",
      "          3.8950e-02,  3.2047e-02,  7.4246e-03,  5.7508e-02,  3.4147e-02,\n",
      "          3.7411e-02,  2.9005e-03,  3.7848e-02,  1.3995e-02,  4.0798e-02,\n",
      "         -3.8110e-03, -7.5618e-03, -1.8903e-02, -4.7563e-02,  1.6325e-02,\n",
      "         -4.7512e-03, -6.0891e-02, -5.8853e-02, -5.0891e-02,  1.0159e-02,\n",
      "          2.9642e-02, -2.0434e-02,  2.0871e-02,  3.4136e-02,  1.7261e-03,\n",
      "          1.8921e-02,  2.5799e-02,  4.6179e-02,  2.2846e-03,  4.8351e-02,\n",
      "          4.8592e-02,  5.1374e-02,  2.3074e-02,  1.9498e-02, -6.4207e-03,\n",
      "         -1.8233e-02, -1.1704e-02, -1.2582e-03,  5.7742e-03, -7.7391e-03,\n",
      "          2.9045e-02,  6.8411e-03, -2.3997e-02, -5.2870e-02, -3.8820e-02,\n",
      "         -6.2319e-02,  6.9928e-03, -7.4651e-03,  3.4213e-02,  6.3331e-03,\n",
      "         -2.5595e-02,  2.3599e-02, -1.8063e-02, -1.4324e-03,  1.2275e-02,\n",
      "          2.5878e-02, -5.2659e-03,  5.5469e-02,  2.8023e-02,  1.3753e-02,\n",
      "          2.0407e-02,  1.2478e-02,  3.4004e-02,  3.0735e-02,  1.1756e-02,\n",
      "          3.0517e-02, -4.4162e-03,  1.1359e-04, -6.7599e-03,  6.2136e-03,\n",
      "          2.2083e-02,  2.3951e-02, -2.7177e-02,  5.7865e-03,  1.0941e-02,\n",
      "          7.0083e-03, -1.4067e-02, -2.8779e-02, -6.2179e-02, -1.4885e-02,\n",
      "          1.4490e-02, -1.0194e-02,  5.3786e-02,  4.7454e-02,  5.6553e-02,\n",
      "          6.9508e-02,  4.3583e-02,  6.1868e-02,  6.4737e-02]])\n",
      "Quantized Weights: tensor([[-0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.7424, -0.7424,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.7424, -0.7424, -0.7424, -0.7424, -0.7424, -0.7424, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.7424, -0.7424, -0.7424, -0.7424,\n",
      "         -0.7424, -0.7424, -0.4862, -0.4862, -0.2300, -0.2300, -0.4862, -0.4862,\n",
      "         -0.7424, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.7424, -0.7424,\n",
      "         -0.7424, -0.7424, -0.7424, -0.7424, -0.7424, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.2300, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.7424, -0.7424, -0.7424, -0.7424, -0.7424, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.2300, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.7424, -0.7424, -0.7424,\n",
      "         -0.4862, -0.4862, -0.4862, -0.7424, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.2300, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.7424, -0.7424, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.2300, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.7424, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.2300, -0.4862, -0.4862, -0.2300, -0.2300, -0.2300,\n",
      "         -0.4862, -0.7424, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.7424, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.2300, -0.2300, -0.4862, -0.4862,\n",
      "         -0.2300, -0.2300, -0.2300, -0.2300, -0.4862, -0.7424, -0.7424, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.2300, -0.4862, -0.4862, -0.4862, -0.2300, -0.2300, -0.2300, -0.2300,\n",
      "         -0.4862, -0.7424, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.7424, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.2300, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.7424, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.2300,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.7424, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.2300,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862,\n",
      "         -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862, -0.4862]])\n"
     ]
    }
   ],
   "source": [
    "# Compare the weights of the original and quantized models\n",
    "for (name, module), (name_q, module_q) in zip(\n",
    "    model.named_modules(), quantized_model_naive.named_modules()\n",
    "):\n",
    "    if name == \"fc1\":\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Original Weights: {module.weight.data[:1]}\")\n",
    "        print(f\"Quantized Weights: {module_q.weight.data[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the reconstructed weights, most of them resulted in the same value, which is due to the fact that we were constrained to using fixed quantization range and needed to account for all the outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 10.04%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the quantized model\n",
    "accuracy_naive = evaluate_model(quantized_model_naive, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy was no better than mere random guess.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model with EasyQuant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without outlier isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing module: fc1\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 128 output channels\n",
      "Quantization completed in 2.56 seconds\n",
      "Quantizing module: fc2\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 64 output channels\n",
      "Quantization completed in 1.52 seconds\n",
      "Quantizing module: fc3\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 10 output channels\n",
      "Quantization completed in 1.50 seconds\n"
     ]
    }
   ],
   "source": [
    "config_easy = EasyQuantConfig(learning_rate=1e-3, num_epochs=100)\n",
    "\n",
    "# Quantize the model\n",
    "\n",
    "quantized_model_easy = quantize_model(\n",
    "    model,\n",
    "    \"EasyQuant\",\n",
    "    config_easy,\n",
    "    num_bits=3,\n",
    "    verbose=True,\n",
    "    retain_outliers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1\n",
      "Original Weights: tensor([[ 0.0450,  0.0145,  0.0132,  0.0735,  0.0662,  0.0706,  0.0331,  0.0595,\n",
      "         -0.0001,  0.0194,  0.0090,  0.0231,  0.0112, -0.0571,  0.0014, -0.0363,\n",
      "         -0.0093,  0.0543,  0.0987,  0.0547]])\n",
      "Quantized Weights: tensor([[ 0.0511,  0.0000,  0.0000,  0.0511,  0.0511,  0.0511,  0.0511,  0.0511,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0511,  0.0000, -0.0511,\n",
      "          0.0000,  0.0511,  0.1023,  0.0511]])\n"
     ]
    }
   ],
   "source": [
    "# Look at the parameters of each layer in the quantized model\n",
    "\n",
    "for (name, module), (name_q, module_q) in zip(\n",
    "    model.named_modules(), quantized_model_easy.named_modules()\n",
    "):\n",
    "    if name == \"fc1\":\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Original Weights: {module.weight.data[:1,:20]}\")\n",
    "        print(\n",
    "            f\"Quantized Weights: {module_q.quantization_executor.reconstruct_layer()[:1,:20]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By optimizing the quantization range to minimize reconstruction error, the reconstructed weights better resemble the original weights than the naive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.90%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the quantized model\n",
    "accuracy_easy = evaluate_model(quantized_model_easy, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 2, Confidence: 0.9493\n"
     ]
    }
   ],
   "source": [
    "# Load a sample image from the test set\n",
    "image, label = test_dataset[0]\n",
    "\n",
    "# Display the image and make a prediction\n",
    "predicted_label, confidence = display_and_predict(quantized_model_easy, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obervation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EasyQuant method (without outlier isolation) was able to achieve a degradation of as little as 2 %.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With outlier isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing module: fc1\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 128 output channels\n",
      "Quantization completed in 2.70 seconds\n",
      "Quantizing module: fc2\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 64 output channels\n",
      "Quantization completed in 1.71 seconds\n",
      "Quantizing module: fc3\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 10 output channels\n",
      "Quantization completed in 1.90 seconds\n"
     ]
    }
   ],
   "source": [
    "quantized_model_easy_out_iso = quantize_model(\n",
    "    model,\n",
    "    \"EasyQuant\",\n",
    "    config_easy,\n",
    "    num_bits=3,\n",
    "    verbose=True,\n",
    "    retain_outliers=True,\n",
    "    outlier_threshold=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1\n",
      "Original Weights: tensor([[ 0.0450,  0.0145,  0.0132,  0.0735,  0.0662,  0.0706,  0.0331,  0.0595,\n",
      "         -0.0001,  0.0194,  0.0090,  0.0231,  0.0112, -0.0571,  0.0014, -0.0363,\n",
      "         -0.0093,  0.0543,  0.0987,  0.0547]])\n",
      "Quantized Weights: tensor([[ 0.0320,  0.0000,  0.0000,  0.0641,  0.0641,  0.0641,  0.0320,  0.0641,\n",
      "          0.0000,  0.0320,  0.0000,  0.0320,  0.0000, -0.0641,  0.0000, -0.0320,\n",
      "          0.0000,  0.0641,  0.0961,  0.0641]])\n"
     ]
    }
   ],
   "source": [
    "# Look at the parameters of each layer in the quantized model\n",
    "\n",
    "for (name, module), (name_q, module_q) in zip(\n",
    "    model.named_modules(), quantized_model_easy_out_iso.named_modules()\n",
    "):\n",
    "    if name == \"fc1\":\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Original Weights: {module.weight.data[:1,:20]}\")\n",
    "        print(\n",
    "            f\"Quantized Weights: {module_q.quantization_executor.reconstruct_layer()[:1,:20]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.14%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the quantized model\n",
    "accuracy_easy_out_iso = evaluate_model(quantized_model_easy_out_iso, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier isolation in this case was able to recover the accuracy by about 1.2 %, which verifies the importance of outlier weights and more optimized quantization range with only normal weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model with SqueezeLLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Sensitivity-based weighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing module: fc1\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 128 output channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deyucao/Quantization/core/models/squeeze_llm.py:287: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:55.)\n",
      "  outlier_weights = outlier_weights.to_sparse_csr()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization completed in 2.78 seconds\n",
      "Quantizing module: fc2\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 64 output channels\n",
      "Quantization completed in 1.56 seconds\n",
      "Quantizing module: fc3\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 10 output channels\n",
      "Quantization completed in 1.74 seconds\n"
     ]
    }
   ],
   "source": [
    "config_squeeze = SqueezeQuantConfig(k_means_max_iter=100, use_sensitivity=False)\n",
    "\n",
    "# Quantize the model\n",
    "\n",
    "quantized_model_squeeze = quantize_model(\n",
    "    model,\n",
    "    \"SqueezeQuant\",\n",
    "    config_squeeze,\n",
    "    num_bits=3,\n",
    "    verbose=True,\n",
    "    retain_outliers=True,\n",
    "    outlier_threshold=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Test Accuracy: 86.51%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the quantized model\n",
    "quantized_model_squeeze.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = quantized_model_squeeze(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Quantized Model Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SqueezeLLM based quantization method (with outlier isolation but without sensitivity-based weighting) led to a decrease in accuracy of about 0.2 %.\\\n",
    "This result is even better than that of the EasyQuant algorithm, which indicates the effectiveness of non-uniform quantization method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1\n",
      "Original Weights: tensor([[ 0.0450,  0.0145,  0.0132,  0.0735,  0.0662,  0.0706,  0.0331,  0.0595,\n",
      "         -0.0001,  0.0194,  0.0090,  0.0231,  0.0112, -0.0571,  0.0014, -0.0363,\n",
      "         -0.0093,  0.0543,  0.0987,  0.0547]])\n",
      "Quantized Normal Weights: tensor([[ 0, -1, -1, -3, -3, -3,  0, -3,  3, -1, -1, -1, -1, -2,  3, -4,  3,  0,\n",
      "          2,  0]], dtype=torch.int8)\n",
      "Reconstructed Weights: tensor([[ 0.0458,  0.0199,  0.0199,  0.0730,  0.0730,  0.0730,  0.0458,  0.0730,\n",
      "         -0.0045,  0.0199,  0.0199,  0.0199,  0.0199, -0.0697, -0.0045, -0.0381,\n",
      "         -0.0045,  0.0458,  0.1136,  0.0458]])\n"
     ]
    }
   ],
   "source": [
    "# Look at the parameters of each layer in the quantized model\n",
    "\n",
    "for (name, module), (name_q, module_q) in zip(\n",
    "    model.named_modules(), quantized_model_squeeze.named_modules()\n",
    "):\n",
    "    if name == \"fc1\":\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Original Weights: {module.weight.data[:1,:20]}\")\n",
    "        print(\n",
    "            f\"Quantized Normal Weights: {module_q.quantization_executor.normal_weights[:1,:20]}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Reconstructed Weights: {module_q.quantization_executor.reconstruct_layer()[:1,:20]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Sensitivity-based weighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing module: fc1\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 128 output channels\n",
      "Quantization completed in 2.80 seconds\n",
      "Quantizing module: fc2\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 64 output channels\n",
      "Quantization completed in 1.66 seconds\n",
      "Quantizing module: fc3\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 10 output channels\n",
      "Quantization completed in 1.60 seconds\n"
     ]
    }
   ],
   "source": [
    "config_squeeze_weighted = SqueezeQuantConfig(k_means_max_iter=100, use_sensitivity=True)\n",
    "\n",
    "# Quantize the model\n",
    "\n",
    "quantized_model_squeeze_weighted = quantize_model(\n",
    "    model,\n",
    "    \"SqueezeQuant\",\n",
    "    config_squeeze_weighted,\n",
    "    num_bits=3,\n",
    "    verbose=True,\n",
    "    retain_outliers=True,\n",
    "    outlier_threshold=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.59%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the quantized model\n",
    "accuracy = evaluate_model(quantized_model_squeeze_weighted, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1\n",
      "Original Weights: tensor([[ 0.0450,  0.0145,  0.0132,  0.0735,  0.0662,  0.0706,  0.0331,  0.0595,\n",
      "         -0.0001,  0.0194,  0.0090,  0.0231,  0.0112, -0.0571,  0.0014, -0.0363,\n",
      "         -0.0093,  0.0543,  0.0987,  0.0547]])\n",
      "Quantized Normal Weights: tensor([[ 3, -2, -2, -3, -3, -3,  3, -3,  0, -2, -2, -2, -2,  1,  0, -4,  0, -3,\n",
      "          2, -3]], dtype=torch.int8)\n",
      "Reconstructed Weights: tensor([[ 0.0388,  0.0143,  0.0143,  0.0648,  0.0648,  0.0648,  0.0388,  0.0648,\n",
      "         -0.0075,  0.0143,  0.0143,  0.0143,  0.0143, -0.0647, -0.0075, -0.0352,\n",
      "         -0.0075,  0.0648,  0.1079,  0.0648]])\n"
     ]
    }
   ],
   "source": [
    "# Look at the parameters of each layer in the quantized model\n",
    "\n",
    "for (name, module), (name_q, module_q) in zip(\n",
    "    model.named_modules(), quantized_model_squeeze_weighted.named_modules()\n",
    "):\n",
    "    if name == \"fc1\":\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Original Weights: {module.weight.data[:1,:20]}\")\n",
    "        print(\n",
    "            f\"Quantized Normal Weights: {module_q.quantization_executor.normal_weights[:1,:20]}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Reconstructed Weights: {module_q.quantization_executor.reconstruct_layer()[:1,:20]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the sensitivity of weights in K-means clustering led to an accuracy improvement of about 0.1% in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with 2-bit representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing module: fc1\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 128 output channels\n",
      "Quantization completed in 2.62 seconds\n",
      "Quantizing module: fc2\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 64 output channels\n",
      "Quantization completed in 1.89 seconds\n",
      "Quantizing module: fc3\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 10 output channels\n",
      "Quantization completed in 1.93 seconds\n"
     ]
    }
   ],
   "source": [
    "config_squeeze_weighted = SqueezeQuantConfig(k_means_max_iter=100, use_sensitivity=True)\n",
    "\n",
    "# Quantize the model\n",
    "\n",
    "quantized_model_squeeze_weighted_2bit = quantize_model(\n",
    "    model,\n",
    "    \"SqueezeQuant\",\n",
    "    config_squeeze_weighted,\n",
    "    num_bits=2,\n",
    "    verbose=True,\n",
    "    retain_outliers=True,\n",
    "    outlier_threshold=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.31%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the quantized model\n",
    "accuracy = evaluate_model(quantized_model_squeeze_weighted_2bit, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 2-bit (4 values) can achieve an almost comparable accuracy as the original float32 model, which is a remarkable result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with 1-bit representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing module: fc1\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 128 output channels\n",
      "Quantization completed in 2.89 seconds\n",
      "Quantizing module: fc2\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 64 output channels\n",
      "Quantization completed in 1.84 seconds\n",
      "Quantizing module: fc3\n",
      "Using torch.int8 for quantization.\n",
      "Quantizing 10 output channels\n",
      "Quantization completed in 1.64 seconds\n"
     ]
    }
   ],
   "source": [
    "# Quantize the model\n",
    "\n",
    "quantized_model_squeeze_weighted_1bit = quantize_model(\n",
    "    model,\n",
    "    \"SqueezeQuant\",\n",
    "    config_squeeze_weighted,\n",
    "    num_bits=1,\n",
    "    verbose=True,\n",
    "    retain_outliers=True,\n",
    "    outlier_threshold=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 41.56%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_model(quantized_model_squeeze_weighted_1bit, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like 2-bit was the bottom line for this model and dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantization-ubRKDRCl-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
